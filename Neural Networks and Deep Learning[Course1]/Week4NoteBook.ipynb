{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep L-Layer Neural Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, the shallow neural network model discussed in the previous sections, will be generalized to a deep network with L layers. Technically, logestic regression is a very shallow neural network. However, a network with quite large L layers is considered to be a deep network. As L get bigger, the network is said to be deeper."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, is a review of the previous notations as well as the new notations associated with notations from the deep L-layers networks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- $L$ refers to the number of layers.\n",
    "- $n^{[l]}$ refers to the number of units in the layer l.\n",
    "- $a^{[l]}$ denotes the activation applied in the layer l. $a^{[0]}$ denotes the inputs.\n",
    "- $w^{[l]}$ represents the weights of layer l.\n",
    "- $b^{[l]}$ represents the biases of layer l."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The general formula for the forward pass"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$ z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]} $$\n",
    "$$ a^{[l]} = g^{[l]}(z^{[l]})$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# L-layers network dimensions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For an L-layers network, the dimension of each of its variables and parameters are as follows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| variable/parameter     | dimension                 | dimension on $m$ training examples  | Notes                                                                                   |\n",
    "| :--------------------: | :-----------------------: | :---------------------------------: | :-------------------------------------------------------------------------------------: |\n",
    "| $$w^{[l]}$$            | $(n^{[l]},(n^{[l-1]})$    | $(n^{[l]},(n^{[l-1]})$              |                                                                                         |\n",
    "| $$\\partial w^{[l]}$$   | $(n^{[l]},(n^{[l-1]})$    | $(n^{[l]},(n^{[l-1]})$              |                                                                                         |\n",
    "| $$b^{[l]}$$            | $(n^{[l]},1)$             | $(n^{[l]},1)$                       | In Python, the vector will be added to the results of $(W.T \\times X)$ via broadcasting |\n",
    "| $$\\partial b^{[l]}$$   | $(n^{[l]},1)$             | $(n^{[l]},1)$                       |                                                                                         |\n",
    "| $$z^{[l]}$$            | $(n^{[l]},1)$             | $(n^{[l]},m)$                       |                                                                                         |\n",
    "| $$\\partial z^{[l]}$$   | $(n^{[l]},1)$             | $(n^{[l]},m)$                       |                                                                                         |\n",
    "| $$a^{[l]}$$            | $(n^{[l]},1)$             | $(n^{[l]},m)$                       | $ a^0 $ refers to the inputs                                                            |\n",
    "| $$\\partial a^{[l]}$$   | $(n^{[l]},1)$             | $(n^{[l]},m)$                       | $ a^0 $ refers to the inputs                                                            |\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Why deep representation?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "earlier layers of the network usually detects general simple features like edges in an image, for instance. Deeper layers compute more complex functions like face eyes, noses itc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An intuition why deep representation works better than shallow representation comes from the circuit theory. In circuit theory, building multi level network of gates to compute basic circuit functions, say the XOR function, turns out to be easily calculated with only $\\mathcal{O}(\\log{n})$ layers. However, it needs an exponentially large number of units if such calculation is restricted to be on only one layer! This is because an $2^{n-1}$ gates are needed to exhaust all the possible configurations of the $n$ inputs. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}