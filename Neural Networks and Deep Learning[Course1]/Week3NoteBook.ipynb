{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Networks Representation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neural Networks are generalization of the previous logestic regression model with multiple layers. Each layer will apply multiple calculations of $\\sigma(z)$ where $z=w^Tx+b$. Consider the following image:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Fig 1: Neural Network Representation](images/neural_network_representation.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It can be noticed that this network can represent a 2-layer neural network. The first layer, layer one, will compute $w^{[1]}.T \\times x + b$, and apply it to the sigmoid $\\sigma $ function. The second layer will compute again $w^{[2]}.T \\times x + b$ and apply $\\sigma $ function to the output to give the final output $\\hat{y}$. It can be seen that each circle does two main computations, the first computation is $z^{[i]}$ and the second computation is $a^{[i]}=\\sigma(z)$ where $i$ represents the layer index. \n",
    "\n",
    "In other words, we can say that the previous network represent a 3 logestic regression units stacked in the first layer, and the output of these units is fed into another one logestic regression unit in the second layer which is responsible of producing the output $\\hat{y}$.\n",
    "\n",
    "\n",
    "The computation of the above network can be summarized in the following computation graph:\n",
    "\n",
    "$$ \\boxed{\\boxed{z^{[1]} = w^{[1]T} \\times x + b^{[1]}} \\longrightarrow \\boxed{a^{[1]} = \\sigma{(z^{[1]})}}} \\longrightarrow \\boxed{\\boxed{z^{[2]}=w^{[2]T} \\times a^{[1]} + b^{[2]}} \\longrightarrow \\boxed{a^{[2]}=\\sigma{(z^{[2]})}}} \\longrightarrow \\boldsymbol{\\ell} (a^{[2]},y)$$\n",
    "\n",
    "- In the above computation graph, outer boxes represents layers while inner boxes represents computations within layers.\n",
    "- **An important note** on the notation is that the superscript square brackets \"$[]$\" will refer to the layer index. This is not to confuse it the parantheses \"$()$\" where they refere to the example index in the given training/dev/test set.\n",
    "- The input layer, $x$ can be also refered to as $ a^{[0]} $.\n",
    "- vectorization and broadcasting techniques can be applied to represent and paralalize the previous network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Activation Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}