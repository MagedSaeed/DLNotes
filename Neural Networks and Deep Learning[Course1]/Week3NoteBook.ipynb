{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Networks Representation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neural Networks are generalization of the previous logestic regression model with multiple layers. Each layer will apply multiple calculations of $\\sigma(z)$ where $z=w^Tx+b$. Consider the following image:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Fig 1: Neural Network Representation](images/neural_network_representation.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It can be noticed that this network can represent a 2-layer neural network. The first layer, layer one, will compute $w^{[1]}.T \\times x + b$, and apply it to the sigmoid $\\sigma $ function. The second layer will compute again $w^{[2]}.T \\times x + b$ and apply $\\sigma $ function to the output to give the final output $\\hat{y}$. It can be seen that each circle does two main computations, the first computation is $z^{[i]}$ and the second computation is $a^{[i]}=\\sigma(z)$ where $i$ represents the layer index. \n",
    "\n",
    "In other words, we can say that the previous network represent a 3 logestic regression units stacked in the first layer, and the output of these units is fed into another one logestic regression unit in the second layer which is responsible of producing the output $\\hat{y}$.\n",
    "\n",
    "\n",
    "The computation of the above network can be summarized in the following computation graph:\n",
    "\n",
    "$$ \\boxed{\\boxed{z^{[1]} = w^{[1]T} \\times x + b^{[1]}} \\longrightarrow \\boxed{a^{[1]} = \\sigma{(z^{[1]})}}} \\longrightarrow \\boxed{\\boxed{z^{[2]}=w^{[2]T} \\times a^{[1]} + b^{[2]}} \\longrightarrow \\boxed{a^{[2]}=\\sigma{(z^{[2]})}}} \\longrightarrow \\boldsymbol{\\ell} (a^{[2]},y)$$\n",
    "\n",
    "- In the above computation graph, outer boxes represents layers while inner boxes represents computations within layers.\n",
    "- **An important note** on the notation is that the superscript square brackets \"$[]$\" will refer to the layer index. This is not to confuse it the parantheses \"$()$\" where they refere to the example index in the given training/dev/test set.\n",
    "- The input layer, $x$ can be also refered to as $ a^{[0]} $.\n",
    "- vectorization and broadcasting techniques can be applied to represent and paralalize the previous network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Activation Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Throughout the previous material, the sigmoid $\\sigma(x) = \\frac{1}{1+e^{-x}}$ function was used to transform the output of the linear regression $wx+b$ into some nonlinar format. However, this is not the only function to use to add some nonlinearity to the linear regression formula. Some other alternative functions are listed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tanh Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tanh function is limited between 1 and -1. It is, technically, a shifted and rescaled version of the sigmoid function. Tanh almost always, for hidden units, works better than the sigmoid function. This is because the mean of its values is closer to zero i.e. it kind of adds a centering effect to data in hand. The formula for the tanh function is:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\\frac{e^{z}-e^{-z}}{e^z+e^{-z}}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following graph, from walfram alpha, depicts the behaviour of the tanh function.\n",
    "\n",
    "![tanh function](images/tanh.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Downsides of tanh and sigmoid functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The most prominent problem with tanh and sigmoid function is that as the input to the function,z, is on its exremes, either small or large, its derivative becomes very small, hence is the gradient. This slows down the learning of the network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The Rectevied Linear Unit ReLU function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ReLU activation function comes mainly to address the limitations of the sigmoid and tanh functions. \n",
    "\n",
    "The ReLU function is defined as follows $$f(x)=\\begin{cases}x & x\\geq 0 \\\\ 0 & x < 0 \\\\ \\end{cases}$$ \n",
    "\n",
    "Hence, ReLU values when the input is on its extremes is not convergin to small values. It becomes the standard practice for the recent neural network implementations. The Graph of the ReLU , from walfram alpha, is illustrated below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![ReLU Graph](images/ReLU.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ReLU limitations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- The function is not differantiable around 0. However, this limitation, in practice, can be mitigated by considering that the function has a 0 or 1 derivative around 0.\n",
    "- The function derivative is always zero for negative inputs. This limitation is amended by another version of the function called leaky ReLU where the function has a tiny slope for the negative input values, say $$f(x)=\\begin{cases}x & x\\geq 0 \\\\ 0.01x & x < 0 \\\\ \\end{cases}$$ \n",
    "Its graph is shown below from paperswithcode.com[https://paperswithcode.com/method/leaky-relu, last-accessed: 31-08-2021]:\n",
    "\n",
    "<img src=\"images/LeakyReLU.png\" width=\"200px\" height=\"200px\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The need for activation functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suppose a neural network does not have any kind of activation function, let us say: an identity activation function. Therefore, it can be noticed that the results of the whole neural network can be achieved through one linear function. Consider the following computation happening accross layers:\n",
    "\n",
    "$$ a^{[1]} = z^{[1]} = w^{[1]}x + b^{[1]} $$\n",
    "$$ a^{[2]} = z^{[2]} = w^{[2]}a^{[1]} + b^{[2]} $$\n",
    "$$ a^{[2]} = w^{[2]}(w^{[1]}x + b^{[1]}) + b^{[2]} $$\n",
    "$$ a^{[2]} = (w^{[2]}w^{[1]})x + (w^{[2]}b^{[1]} + b^{[2]}) $$\n",
    "\n",
    "let $$w^`= (w^{[2]}w^{[1]})$$\n",
    "and $$b^`=(w^{[2]}b^{[1]} + b^{[2]})$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$ a^{[2]} = w^`x + b^`$$\n",
    "\n",
    "The above computation was applied on two layers network, but can be, similarly, generalized to k layers network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It turns out that having a linear activation function for the hidden layers, the neural network is no different than a standard logestic regression model without any hidden layer. The above computation gives some insights on this conclusion."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Hence,** a non-linear activation function is required to transform the regression function $wx+b$ into some nonlinear form."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}