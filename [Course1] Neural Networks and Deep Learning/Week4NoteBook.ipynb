{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep L-Layer Neural Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, the shallow neural network model discussed in the previous sections, will be generalized to a deep network with L layers. Technically, logestic regression is a very shallow neural network. However, a network with quite large L layers is considered to be a deep network. As L get bigger, the network is said to be deeper."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, is a review of the previous notations as well as the new notations associated with notations from the deep L-layers networks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- $L$ refers to the number of layers.\n",
    "- $n^{[l]}$ refers to the number of units in the layer l.\n",
    "- $a^{[l]}$ denotes the activation applied in the layer l. $a^{[0]}$ denotes the inputs.\n",
    "- $w^{[l]}$ represents the weights of layer l.\n",
    "- $b^{[l]}$ represents the biases of layer l."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The general formula for the forward pass"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$a^{[l]} = g^{[l]}(z^{[l]})$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# L-layers network dimensions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For an L-layers network, the dimension of each of its variables and parameters are as follows."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "| variable     | dimension                 | dimension on $m$ training examples  | Notes                                                                                   |\n",
    "| :--------------------: | :-----------------------: | :---------------------------------: | :-------------------------------------------------------------------------------------: |\n",
    "| $$w^{[l]}$$            | $(n^{[l]},(n^{[l-1]})$    | $(n^{[l]},(n^{[l-1]})$              |                                                                                         |\n",
    "| $$\\partial w^{[l]}$$   | $(n^{[l]},(n^{[l-1]})$    | $(n^{[l]},(n^{[l-1]})$              |                                                                                         |\n",
    "| $$b^{[l]}$$            | $(n^{[l]},1)$             | $(n^{[l]},1)$                       | In Python, the vector will be added to the results of $(W.T \\times X)$ via broadcasting |\n",
    "| $$\\partial b^{[l]}$$   | $(n^{[l]},1)$             | $(n^{[l]},1)$                       |                                                                                         |\n",
    "| $$z^{[l]}$$            | $(n^{[l]},1)$             | $(n^{[l]},m)$                       |                                                                                         |\n",
    "| $$\\partial z^{[l]}$$   | $(n^{[l]},1)$             | $(n^{[l]},m)$                       |                                                                                         |\n",
    "| $$a^{[l]}$$            | $(n^{[l]},1)$             | $(n^{[l]},m)$                       | $a^0$ refers to the inputs                                                            |\n",
    "| $$\\partial a^{[l]}$$   | $(n^{[l]},1)$             | $(n^{[l]},m)$                       | $a^0$ refers to the inputs                                                            |\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Why deep representation?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "earlier layers of the network usually detects general simple features like edges in an image, for instance. Deeper layers compute more complex functions like face eyes, noses itc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "An intuition why deep representation works better than shallow representation comes from the circuit theory. In circuit theory, building multi level network of gates to compute basic circuit functions, say the XOR function, turns out to be easily calculated with only $\\mathcal{O}(\\log{n})$ layers. However, it needs an exponentially large number of units if such calculation is restricted to be on only one layer! This is because an $2^{n-1}$ gates are needed to exhaust all the possible configurations of the $n$ inputs. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameters vs Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parameters of the neural network model are $w$ and $b$. However, there are many other numbers that need to be set. For instance:\n",
    "\n",
    "- Learning rate $\\alpha$.\n",
    "- Number of iterations.\n",
    "- Number of hidden layers L.\n",
    "- Number of hidden units $n^{[l]}$ in the layer l.\n",
    "- Choice of activation function.\n",
    "\n",
    "These are some hyperparameters up to the current materials. However, there are other hyperparameters that will come on the way like the momentum term, minibatch size, regularization parameters, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The current practice of applied deep learning is a very empirical process. The practitioner will start first with the idea, write the code, conduct the experiment, reiterate on the code and the hyperparameters, experiment again and so on until reaching to a the sought results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}