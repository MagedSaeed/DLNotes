{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train, Dev and Test sets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As previously mentioned, training a neural network is a very iterative process. The are many hyperparameters that needs to be chosen and adjusted iteratively. To take the most advantage of the data and find out the optimal values for the hyperparameters, the dataset can be split into three sets:\n",
    "- The training set. This is used to train the algorithm and its hyperparameters.\n",
    "- The hold out/cross validation/development set. This is used to test the algorithm and its parameters while searching for the optimal hyperparameters. This test set will be used multiple times through out the iterative process of network training.\n",
    "- Once hyperparameters are found and the algorithm stabilizes, it is then tested on a completely separate test set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, the workflow is to train algorithms and their parameters on the training set, test their performance and compare them on the development set. Finally, once the final model is found, it is then evaluated on the test set in order to get an unbiased estimate of the algorithm's performance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train, dev, and test sets sizes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Previously, the ration of 70%/30% splits for train/test sets or maybe 60%/20%/20% splits for train/development/test splits was widely considered the best practice. However, these ratios was applied on a small datasets. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On the modern deep learning era, the size of the datasets is extremely large. Having a very large test set may reduce the possibility to better exploit the data for further variations and patterns. Hence, usually, the ratio becomes that around 99% of the dataset is reserved for training, 1% for development, and 1% for test sets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One important remark to consider is to keep an eye on **the train/test sets distributions**. Both sets should come from the same distribution. For example, if the network is trained on a high quality images, the test set be of high quality images as well."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, it might be quite enough to have only train and test set. However, in this case, the algorithm might suffer some bias as it is tested on the same development set used to compare its performance with other algorithms and hyperparameters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bias/Variance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Bias and variance are rough metrics that describes the performance of the learning algorithm. To illustrate more, consider the following graphs from AndrewNG's class:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![bias and virance](images/bias_variance.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- If the algorithm provides a shallow fit for the data, say a linear fit, then the algorithm will perform poor on the training set. In this case, it is said to have high bias. Another term that can be used is that the algorithm underfits the data.\n",
    "- If the algorithm draws a sophisticated classifier to fit the data, it may be very hard to generalize well. In this case, it will perform well on the training set, but it will perform poorly on the development set. In this case, the algorithm is said to have a high variance. Another term that can be used is that the algorithm overfits the data.\n",
    "- If the algorithm performs well on both, the training and development set, then it is said to have low bias and low variance.\n",
    "- In some cases, the algorithm may underfit most of the data and overfits some portions of the data. In this case, the training error will be quite high and the development error mostly is higher. In this case, the algorithm is said to have high variance and high bias."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above analysis assumes that the optimal error, for example, human classification error, is close to zero."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bias and Variance analysis, how to approach? \"Basic recipe for machine learning\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This subsection provides a basic recipe to deal with the variance/bias problems."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- high bias: \n",
    "  - Try a bigger network.\n",
    "  - Train for more time.\n",
    "  - Try another deep architectures.\n",
    "- high variance:\n",
    "  - Get more data.\n",
    "  - Regularization.\n",
    "  - Try some other alternative architectures."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bias/Variance tradeoff"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Previously, there was a lot of discussion about what is called bias/variance tradeoff. it was very hard to reduce bias/variance without hurting the other one. However, in the modern days, a plenty amount of data and various algorithms are available on the shelf for many problems. It was not a strict correlation between variance and bias any more"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regularization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One of the useful techniques to reduce variance/overfitting is to apply regularization to the neural network model. Regularization adds a penalty to the cost function. This penalty, consequently, propagates to the model's weights updates. The added term positively correlates to the weights themselves."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Types of regularization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the added term to the cost function, regularization can be of multiple types. Usually, some hyper parameters are added to add further flexibility. Usually, a hyperparameter $\\frac{\\lambda}{2m}$ is applied for all types."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- L2 norm regularization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The added term here is the Frobenius form of the weights matrix. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$||w||^2_2 = \\frac{\\lambda}{2m} \\sum^{n_x}_{j=1} w_j^2 = w^Tw$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Although this type is widely known as $L_2$ regularization, for some arcane reasons in linear algebra. It is called the Frobenius form which means just the sum of the square elements of the matrix. This type of regularization is also referred to as **Weight Decay**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The weights update of this norm is $$\\partial w = \\alpha \\partial w + \\frac{\\lambda}{m}w^{[l]}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hence, the final update of the weights matrix is :"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$w^{[l]} = w^{[l]} - \\alpha [\\partial w + \\frac{\\lambda}{m} w^{[l]}]$$\n",
    "$$w^{[l]} = w^{[l]} - \\frac{\\alpha \\lambda}{m} w^{[l]} - \\alpha (\\partial w)$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- L1 norm regularization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The added term is:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$||w||_1 = \\frac{\\lambda}{2m} \\sum^{n_x}_{j=1} |w|$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This type of regularization makes the weights matrix sparse. It is claimed that such sparsity benefits the model with compressing the model as less memory will be consumed. However, from practical point of view, this might not be much useful. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Why do we only regularize the weights matrix but not the biases?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Technically, biases can be regularized as well. However, as it is only a single parameter, it wont contribute as much as the weights matrix."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Why does regularization helps with overfitting?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Basically, what regularization does is to decrease the neurons values shrinking them to be closer to zero, something like shutting down a neuron. This also results in a smaller and simpler neural network going to lower level closer to a logestic regression model. Such process reduce the effectiveness of these neurons which results, generally, in variance reduction and learning a function with less pattern complexity."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another intuition why such regularization works may come clearer when using tanh or sigmoid activation functions. These functions tend to be linear around zero. Hence, pushing weights to be around zero approximates the affected neuron results to be linear. As previously mentioned, having neurons that computes a linear function can be computed with only one single logestic regression unit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dropout regularization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the purpose of regularization is to reduce the effect of some neurons, why not, then, shuting down some of these neurons explicitly instead of adding regularization forms to the cost function? This idea is the basic motivation behind dropout regularization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on a probability parameter, p, that is added to the model, neurons in layer l has p probability of being active and 1-p of being shut down. Increasing p indicates higher regularization. So the result, will be much smaller, much diminished neural network. The dropout probability p can be configured on each layer based on the overfitting effect anticipated on that layer. Usually, large layers are usually more subjective to overfitting and, hence, they usually have lower dropout probability p."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keep in mind that neurons killed on random on gradient descent iteration j, are different from those neurons killed at iteration i, where $i \\ne j$. This reduces the coupling of features affecting/dominating some specific neurons in different layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One of the popular techniques to implement dropout is the inverted dropout. In this technique, the output of the neuron at layer l, $a^{[l-1]}$ is divided by the dropout probability, p. This is to preserve the expected value of $a^{[l-1]}$ along the consequent layers computations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure that dropout **should not** be used on test time. This is because it adds randomness to the outputs and unnecessary noise to the model. Maybe, dropout can be used on test time on a large number of iterations then the final results will be the average of such iterations. However, this process is computationally infeasible and impractical."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One key intuition why dropout works is it releases neurons from being dependent on dominating feature. Hence, weights have to be spread out leading to weights shrinking which is the same effect produced by $l_2$ regularization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One big downside of dropout is that the cost function $J$ is no longer well-defined. This is due to the randomness of shutting down neurons on different iterations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Other regularization techniques"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## getting more synthetic data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since getting more data can be useful to reduce overfitting, adding data mostly add values to the model. Obtaining data in some cases can be costly. However, data augmentation can be a good tool. Data augmentation refers to generating data items from the data itself by applying some transformations like, in case of images, rotation, horizontal flips, light blurring and zooming in/out."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## early stopping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "one of the key metric for analyzing the bias/variance tradeoff is to monitor the training error/loss and the development error/loss vs the number of iterations. Usually, both of them  tend to reduce on the initial runs of the model. However, the development set errors goes up in the case of overfitting in contrast to the training error which continues to go down as depicted in the following image from the Andrew Ng's lecture below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![early stopping graph](images/early_stopping.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "early stopping technique stops the training at the mid-size square training weights to minimize the effect of overfitting."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The downside of this early stopping is that it may not allow the learning algorithm to fully exploit the whole data. Usually, regularization happens after optimizing the cost function. However, early stopping couples both tasks at once giving limited flexibility to try various tools for cost function optimization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Some pitfalls on setting a well optimization problem"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input normalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Input normalization corresponds to two steps:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- mean normalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This step normalizes inputs to have zero mean. This can be achieved with the following formula"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\\mu = \\frac{1}{m} \\sum^{m}_{i=1}x^{(i)}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, for each $x$ in the training examples set:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$x=x-\\mu$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- variance normalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly as being done with mean normalization, variance can be normalized as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\\sigma ^2 = \\frac{1}{m} \\sum^{m}_{i=1} x^{(i)^2}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, for each example $x$ in the training set:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$x= \\frac{x}{\\sigma}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One more **important remark**, is to use these two same parameters $\\sigma$ and $\\mu$ to scale the test set as well."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Why input normalization is useful?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the input features come from different scales, say $x_1$ ranges from 0 to 1 and $x_2$ ranges from 0 to 100 for a problem with two features, then the cost function will be elongated squashed out. However, once these features are centered around their mean and variance the cost function becomes more closer to have a rounded uniform, symmetric contours. Such function needs less iterations to reach its minimum via gradient descent with even larger learning rate. Consider the following image taken from Andrew Ng's course depicting such cases."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![nomralized vs unnormalized inputs](images/nomalized_vs_unnormalized_inputs.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vanishing/Exploding graidents"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This pitfall refers to situation where gradients goes extremely small or extremely large. This situation makes the process harder and difficult."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see how this happens, consider a linear activation function with weights initialized to values close to the identity matrix. The final prediction $\\hat{y}$, as the network goes deeper, goes exponentially large if the weights are slightly greater than the identity matrix or exponentially small if the weights are slightly smaller than the identity matrix."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One method to approach this issue is to generate random weights in layer, $l$, where these weights have $\\frac{1}{n_x}$ variance $n_l$ where $n_l$ is the inputs of layer $l$. The intuition behind this method is that, as $n_x$ goes larger, weights in the layer goes smaller. The denominator in the term $\\frac{1}{n_x}$ can varies according to the activation function used."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient checking"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gradient checking is a method that gives an insights on the gradient correctness. This happens to help as a debugging tool to make sure that the gradient implementation is not buggy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a neural network NN with parameters $w_1,b_1,w_2,b_2,\\dots,w_i,b_i,\\dots,w_n,b_n=\\theta_1+\\theta_2+\\theta_3+\\theta_4+\\dots+\\theta_i+\\theta_i+1+\\dots+\\theta_{2n-1}+\\theta_{2n}=\\Theta$ and cost function $J(\\Theta)$ where $n$ is the number of layers, we can check gradients using by approximating its two sided difference derivatives then comparing such approximation to the resulted gradients."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " To compute the approximated two sided difference gradient approximation for layer $\\theta_i$:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\\partial \\theta_{[i]} = \\frac{J(\\theta_1+\\theta_2+\\dots+\\theta_{i+\\epsilon}+\\theta_i+1+\\dots+\\theta_{2n})-J(\\theta_1+\\theta_2+\\dots+\\theta_{i-\\epsilon}+\\theta_i+1+\\dots+\\theta_{2n})}{2\\epsilon}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This should be approximately equal to its gradient with respect to $\\theta_i$:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\\partial \\theta_{[i]} \\approx \\frac{\\partial J}{\\partial \\theta_i}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To check for equality of $\\partial \\theta_{[i]}$ and $\\frac{\\partial J}{\\partial \\theta_i}$, as they are just vectors, we can find their the euclidean distance, the $l_2$ norm, and normalize it by the euclidean length of each vector $\\partial \\theta_{[i]}$ and $\\frac{\\partial J}{\\partial \\theta_i}$. More formally, the gradient check can be stated as:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\\frac{||\\partial \\theta_{[i]} - \\frac{\\partial J}{\\partial \\theta_i}||_2}{||\\partial \\theta_{[i]}||_2 + ||\\frac{\\partial J}{\\partial \\theta_i}||_2}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Where, for a vector $Y$, the euclidean length, $l_2$ norm, of that vector is:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$||Y||_2 = \\sqrt{\\sum_{i=1}^{n}Y_i^2}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "more information can be found [here](https://www.cs.utexas.edu/users/flame/laff/alaff/chapter01-vector-2-norm.html): [https://www.cs.utexas.edu/users/flame/laff/alaff/chapter01-vector-2-norm.html]."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the result of checking formula is approximately equal to $\\epsilon$ or less, then this is an indication that the gradients are mostly calculated correctly for the parameter $\\theta_i$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Some notes on gradient checking"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Gradient checking, as it is costly, is used only to debug, not in training. \n",
    "- "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}